{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ニュース一覧ページのURL\n",
    "url = \"https://www.kiramex.com/news/\"\n",
    "\n",
    "# ページのHTMLを取得\n",
    "response = requests.get(url)\n",
    "response.encoding = response.apparent_encoding  # 文字コードを自動判別して設定\n",
    "\n",
    "# HTMLをBeautifulSoupでパース\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# ニュース一覧の情報を抽出\n",
    "news_list = soup.find_all(\"li\", class_=\"news-block\")\n",
    "\n",
    "# ニュース情報の取得\n",
    "for news in news_list:\n",
    "    # ニュースの日付を取得\n",
    "    date = news.find(\"p\", class_=\"date\").get_text(strip=True) if news.find(\"p\", class_=\"date\") else \"N/A\"\n",
    "\n",
    "    # ニュースのタイトルを取得\n",
    "    title = news.find(\"a\", class_=\"title\").get_text(strip=True)\n",
    "\n",
    "    # ニュースのリンクを取得\n",
    "    link = news.find(\"a\", class_=\"title\")[\"href\"]\n",
    "\n",
    "    print(f\"Date: {date}\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Link: {link}\")\n",
    "    print(\"-\" * 40)"
   ],
   "id": "41d37b065a51dd0d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 必要なモジュールをインポート\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "# 環境変数の取得\n",
    "load_dotenv(\"../.env\")\n",
    "\n",
    "# OpenAI APIクライアントを生成\n",
    "client = OpenAI(api_key=os.environ['API_KEY'])\n",
    "\n",
    "# モデル名\n",
    "MODEL_NAME = \"gpt-4o-mini\""
   ],
   "id": "18823c36d8143107",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ニュース一覧ページのURL\n",
    "url = \"https://www.kiramex.com/news/\"\n",
    "\n",
    "# ページのHTMLを取得\n",
    "response = requests.get(url)\n",
    "response.encoding = response.apparent_encoding  # 文字コードを自動判別して設定\n",
    "\n",
    "# HTMLをBeautifulSoupでパースし、body部分を取り出す\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "body_html = str(soup.body)  # body部分のHTMLを文字列として取得\n",
    "print(body_html) # 結果を表示して確認"
   ],
   "id": "d6e2208f9bdcc584",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# LLMにニュース一覧を抽出させるプロンプトを作成\n",
    "prompt = f\"\"\"\n",
    "以下のHTMLから最新のニュースを抽出し、「日付、タイトル、リンク」の形式で一覧を出力してください。一覧以外は出力しないでください。\n",
    "\n",
    "# 出力様式：\n",
    "Date: 日付\n",
    "Title: タイトル\n",
    "Link: リンク\n",
    "--------------------\n",
    "\n",
    "#HTML:\n",
    "{body_html[:5000]}\n",
    "\"\"\"\n",
    "\n",
    "# APIへリクエスト\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    max_tokens=500,\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# LLMからの回答を表示\n",
    "print(response.choices[0].message.content.strip())"
   ],
   "id": "3099d830d16afe12",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
